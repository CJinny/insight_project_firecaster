{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "insight_cnn_lstm_4.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwzYUmvpzw-j",
        "colab_type": "code",
        "outputId": "ff3693f7-0d2a-4db2-af98-354a6402f01c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import matplotlib\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, TensorBoard\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dropout, UpSampling2D, BatchNormalization, Activation, Dense, Lambda, TimeDistributed, Flatten\n",
        "from keras.layers import LSTM, GRU, RNN, Reshape, CuDNNGRU, CuDNNLSTM\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D, MaxPooling3D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv3D\n",
        "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
        "from keras import backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import Callback\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "from keras import layers, models, applications\n",
        "from keras.layers import concatenate\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras import layers, models, applications\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "from keras.backend.tensorflow_backend import clear_session\n",
        "from keras.backend.tensorflow_backend import get_session\n",
        "import tensorflow\n",
        "# Reset Keras Session\n",
        "def reset_keras():\n",
        "    sess = get_session()\n",
        "    clear_session()\n",
        "    sess.close()\n",
        "    sess = get_session()\n",
        "    try:\n",
        "        del classifier # this is from global space - change this as you need\n",
        "    except:\n",
        "        pass\n",
        "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
        "\n",
        "    # use the same config as you used to create the session\n",
        "    config = tensorflow.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
        "    config.gpu_options.visible_device_list = \"0\"\n",
        "    set_session(tensorflow.Session(config=config))\n",
        "\n",
        "def random_everything(seed=42):\n",
        "  os.environ['PYTHONHASHSEED'] = str(42)\n",
        "  random.seed(42)\n",
        "  np.random.seed(42)\n",
        "  tf.set_random_seed(42)\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f2_m(y_true, y_pred, beta=2):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return (1+beta**2) * (precision*recall) / ((beta**2)*(precision)+recall+K.epsilon())\n",
        "\n",
        "def f3_m(y_true, y_pred, beta=3):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return (1+beta**2) * (precision*recall) / ((beta**2)*(precision)+recall+K.epsilon())\n",
        "\n",
        "def f4_m(y_true, y_pred, beta=4):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return (1+beta**2) * (precision*recall) / ((beta**2)*(precision)+recall+K.epsilon())\n",
        "    \n",
        "'''\n",
        "a = []\n",
        "while(1):\n",
        "  a.append('1')\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\na = []\\nwhile(1):\\n  a.append('1')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogelmAZAz5k7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ConvBlock(inputs, suffix=0, prefix=\"cnn\"):\n",
        "    \"\"\"no final linear dense layer!\"\"\"\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_1a\")(inputs)\n",
        "    conv1 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_1a\")(conv1)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_1b\")(conv1)\n",
        "    conv1 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_1b\")(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), name=f\"{prefix}_{suffix}_maxpool_1\")(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_2a\")(pool1)\n",
        "    conv2 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_2a\")(conv2)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_2b\")(conv2)\n",
        "    conv2 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_2b\")(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), name=f\"index_{suffix}_maxpool_2\")(conv2)\n",
        "    \n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_3a\")(pool2)\n",
        "    conv3 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_3a\")(conv3)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_3b\")(conv3)\n",
        "    conv3 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_3b\")(conv3)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_3c\")(conv3)\n",
        "    conv3 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_3c\")(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), name=f\"index_{suffix}_maxpool_3\")(conv3)\n",
        "\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_4a\")(pool3)\n",
        "    conv4 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_4a\")(conv4)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_4b\")(conv4)\n",
        "    conv4 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_4b\")(conv4)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_4c\")(conv4)\n",
        "    conv4 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_4c\")(conv4)    \n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2), name=f\"{prefix}_{suffix}_maxpool_4\")(conv4)    \n",
        "\n",
        "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_5a\")(pool4)\n",
        "    conv5 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_5a\")(conv5)\n",
        "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_5b\")(conv5)\n",
        "    conv5 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_5b\")(conv5)\n",
        "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=f\"{prefix}_{suffix}_conv_5c\")(conv5)\n",
        "    conv5 = BatchNormalization(name=f\"{prefix}_{suffix}_batchnorm_5c\")(conv5)    \n",
        "    pool5 = MaxPooling2D(pool_size=(2, 2), name=f\"{prefix}_{suffix}_maxpool_5\")(conv5)   \n",
        "    \n",
        "    flatten= Flatten(name=f\"{prefix}_{suffix}_flatten\")(pool5)\n",
        "    dense1 = Dense(512, activation=\"relu\", name=f\"{prefix}_{suffix}_relu_1\")(flatten)\n",
        "    drop1  = Dropout(0.4, name=f\"{prefix}_{suffix}_drop_1\")(dense1)\n",
        "    dense2 = Dense(128, activation=\"relu\", name=f\"{prefix}_{suffix}_relu_2\")(drop1)\n",
        "    drop2  = Dropout(0.3, name=f\"{prefix}_{suffix}_drop_2\")(dense2)\n",
        "    dense3 = Dense(32, activation=\"relu\", name=f\"{prefix}_{suffix}_relu_3\")(drop2)\n",
        "    drop3  = Dropout(0.2, name=f\"{prefix}_{suffix}_drop_3\")(dense3)\n",
        "    dense4 = Dense(8, activation=\"relu\",  name=f\"{prefix}_{suffix}_relu_4\")(drop3)\n",
        "    return dense4\n",
        "\n",
        "def MLP(inputs, suffix=0, prefix='mlp'):\n",
        "    dense = Dense(8, activation='relu', name=f\"{prefix}_{suffix}_relu_1\")(inputs)\n",
        "    dense = Dense(4, activation='relu', name=f\"{prefix}_{suffix}_relu_2\")(dense)\n",
        "    return dense\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvCm3POTz7_6",
        "colab_type": "code",
        "outputId": "0a43beda-125f-444f-d614-57010334b40f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "n_channels = 14\n",
        "sector_dim = 200\n",
        "\n",
        "img_0 = Input((sector_dim,sector_dim,n_channels), name=\"cnn_input_0\")\n",
        "img_1 = Input((sector_dim,sector_dim,n_channels), name=\"cnn_input_1\")\n",
        "img_2 = Input((sector_dim,sector_dim,n_channels), name=\"cnn_input_2\")\n",
        "\n",
        "num_0 = Input((50,), name=\"gsod_input_0\")\n",
        "num_1 = Input((50,), name=\"gsod_input_1\")\n",
        "num_2 = Input((10,), name=\"gsod_input_2\")\n",
        "\n",
        "adj_0 = Input((8,), name=\"adj_input_0\")\n",
        "adj_1 = Input((8,), name=\"adj_input_1\")\n",
        "adj_2 = Input((8,), name=\"adj_input_2\")\n",
        "\n",
        "cnn_0 = ConvBlock(img_0, suffix=0)\n",
        "cnn_1 = ConvBlock(img_1, suffix=1)\n",
        "cnn_2 = ConvBlock(img_2, suffix=2)\n",
        "\n",
        "num_0_m = MLP(num_0, suffix=0, prefix=\"gsod\")\n",
        "num_1_m = MLP(num_1, suffix=1, prefix=\"gsod\")\n",
        "num_2_m = MLP(num_2, suffix=2, prefix=\"gsod\")\n",
        "\n",
        "adj_0_m = MLP(adj_0, suffix=0, prefix=\"adj\")\n",
        "adj_1_m = MLP(adj_1, suffix=1, prefix=\"adj\")\n",
        "adj_2_m = MLP(adj_2, suffix=2, prefix=\"adj\")\n",
        "\n",
        "mixed_0 = concatenate([cnn_0, num_0_m, adj_0_m], name='mixed_0')\n",
        "mixed_1 = concatenate([cnn_1, num_1_m, adj_1_m], name='mixed_1')\n",
        "mixed_2 = concatenate([cnn_2, num_2_m, adj_2_m], name='mixed_2')\n",
        "\n",
        "concat   = concatenate([mixed_0, mixed_1, mixed_2], name=\"concat_mixed\")\n",
        "reshape  = Reshape((3, -1), name=\"reshape\")(concat)\n",
        "lstm     = LSTM(256, name='lstm_1', return_sequences=True)(reshape)\n",
        "lstm     = Dropout(0.4, name='lstm_drop_1')(lstm)\n",
        "lstm     = LSTM(256, name='lstm_2')(lstm)\n",
        "lstm     = Dropout(0.4, name='lstm_drop_2')(lstm)\n",
        "dense    = Dense(10, activation=\"relu\", name=\"fc_relu\")(lstm)\n",
        "dense    = Dense(1, activation=\"sigmoid\", name=\"fc_sigmoid\")(dense)\n",
        "\n",
        "model = Model(inputs=[img_0, img_1, img_2, num_0, num_1, num_2, adj_0, adj_1, adj_2], output=dense)\n",
        "#model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"fc...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNArw5jvz9k2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data(object):\n",
        "    def __init__(self, imgs, labs, zone_idx=np.arange(20), size=108):\n",
        "        self.imgs = imgs\n",
        "        self.labs = labs\n",
        "        self.size = size\n",
        "        self.zone_idx = zone_idx\n",
        "\n",
        "    def extract_single_zone(self, imgs, labs, idx=0):\n",
        "        \"\"\"extract a single zone based on the index\"\"\"\n",
        "        assert idx >= 0 and idx <= 24\n",
        "        try:\n",
        "            assert len(imgs) == self.size\n",
        "        except Exception:\n",
        "            imgs = imgs[:self.size,:]     ## if there are more images than labels, cut the images to approporiate size!\n",
        "        try:\n",
        "            assert len(labs) == self.size\n",
        "        except Exception:\n",
        "            labs = labs[:self.size,:]     \n",
        "\n",
        "        row_idx = idx//5\n",
        "        col_idx = idx%5\n",
        "        return imgs[:, row_idx*200:(row_idx+1)*200, col_idx*200:(col_idx+1)*200, :], labs[:, row_idx*200:(row_idx+1)*200, col_idx*200:(col_idx+1)*200, :]\n",
        "   \n",
        "    def burn_ratio(self, labs):\n",
        "        \"\"\"calculate the burn ratio in an area\"\"\"\n",
        "        ratio = np.zeros((labs.shape[0],))\n",
        "        for i in range(len(labs)):\n",
        "          ratio[i] = labs[i,:,:,:].mean()\n",
        "        return ratio\n",
        "\n",
        "    def binarize_risk(self, labs, thres=0.05):\n",
        "        #ratio = labs.sum(axis=-1).sum(axis=-1).sum(axis=-1)/(200*200)\n",
        "        ratio = self.burn_ratio(labs)\n",
        "        return (ratio>thres).astype(int)\n",
        "\n",
        "    def compile_zones(self):\n",
        "        \"\"\"concatenate all sector images based on selected index\"\"\"\n",
        "        \"\"\"Zone 0,0,0,0,0,0,0,0,0 .... Zone 1,1,1,1....\"\"\"\n",
        "        \"\"\"Normalize to (0, 1) !!!\"\"\"\n",
        "        imgs, labs = self.extract_single_zone(self.imgs, self.labs, idx=self.zone_idx[0])\n",
        "        for i in range(1, len(self.zone_idx)):\n",
        "            I, L = self.extract_single_zone(self.imgs, self.labs, idx=self.zone_idx[i])\n",
        "            imgs = np.concatenate([imgs, I], 0)\n",
        "            labs = np.concatenate([labs, L], 0)\n",
        "        imgs = imgs/imgs.max()\n",
        "        imgs = imgs.astype('float32')\n",
        "        return imgs, labs\n",
        "        \n",
        "    def prepare_step_data(self):\n",
        "        zoned_imgs, zoned_labs = self.compile_zones()\n",
        "        idx = np.array([i for i in range(len(zoned_imgs)) if i % self.size not in [self.size-2, self.size-1, self.size]])\n",
        "        imgs_0 = zoned_imgs[idx, :,:,:]\n",
        "        imgs_1 = zoned_imgs[idx+1, :,:,:]\n",
        "        imgs_2, labs = zoned_imgs[idx+2, :,:,:], self.binarize_risk(zoned_labs[idx+2])\n",
        "        return imgs_0, imgs_1, imgs_2, labs\n",
        "\n",
        "\n",
        "class Adjacency(object):\n",
        "    \"\"\"Only to extract burn ratio from label, no images, no binarized labels, burn ratio is the feature to extract\"\"\"\n",
        "    def __init__(self, labs, zone_idx=np.arange(20), size=108):\n",
        "        self.labs = labs\n",
        "        self.size = size\n",
        "        self.zone_idx = zone_idx\n",
        "    \n",
        "    def extract_single_zone(self, labs, idx=0):\n",
        "        \"\"\"extract a single zone based on the index\"\"\"\n",
        "        assert idx >= 0 and idx <= 24\n",
        "        try:\n",
        "            assert len(labs) == self.size\n",
        "        except Exception:\n",
        "            labs = labs[:self.size,:]     \n",
        "        row_idx = idx//5\n",
        "        col_idx = idx%5\n",
        "        return labs[:, row_idx*200:(row_idx+1)*200, col_idx*200:(col_idx+1)*200, :]\n",
        "\n",
        "    def burn_ratio(self,labs):\n",
        "        \"\"\"calculate the burn ratio in an area\"\"\"\n",
        "        ratio = np.zeros((labs.shape[0],))\n",
        "        for i in range(len(labs)):\n",
        "          ratio[i] = labs[i,:,:,:].mean()\n",
        "        return ratio\n",
        "    \n",
        "    def binarize_risk(self,labs, thres=0.05):\n",
        "        ratio = labs.sum(axis=-1).sum(axis=-1).sum(axis=-1)/(200*200)\n",
        "        return (ratio>thres).astype(int)\n",
        "\n",
        "    def compile_zones(self):\n",
        "        \"\"\"concatenate all sector images based on selected index\"\"\"\n",
        "        \"\"\"Zone 0,0,0,0,0,0,0,0,0 .... Zone 1,1,1,1....\"\"\"\n",
        "        \"\"\"Normalize to (0, 1) !!!\"\"\"\n",
        "        labs = self.extract_single_zone(self.labs, idx=self.zone_idx[0])\n",
        "        for i in range(1, len(self.zone_idx)):\n",
        "            L = self.extract_single_zone(self.labs, idx=self.zone_idx[i])\n",
        "            labs = np.concatenate([labs, L], 0)\n",
        "        return labs\n",
        "\n",
        "    def prepare_step_data(self):\n",
        "        zoned_labs = self.compile_zones()\n",
        "        idx = np.array([i for i in range(len(zoned_labs)) if i % self.size not in [self.size-2, self.size-1, self.size]])\n",
        "        adj_0 = self.burn_ratio(zoned_labs[idx])\n",
        "        adj_1 = self.burn_ratio(zoned_labs[idx+1])\n",
        "        adj_2 = self.burn_ratio(zoned_labs[idx+2])\n",
        "        return adj_0, adj_1, adj_2\n",
        "\n",
        "def extract_adjacency_feature(indices, mode='trn_val'):\n",
        "    \"\"\"\n",
        "    Function to extract adjacency features\n",
        "    indices must be in list format,\n",
        "    mode can be either trn_val or test\n",
        "    \"\"\"\n",
        "    names = ['south','southwest','west','northwest','north','northeast','east','southeast']\n",
        "    \n",
        "    def subset_adjacency(which=0):\n",
        "        \"\"\"Extract adjacency feature based on 1 direction\"\"\"\n",
        "        name = names[which]\n",
        "        row = 200\n",
        "        col = 200\n",
        "        if 'south' in name:\n",
        "            row = 400\n",
        "        elif 'north' in name:\n",
        "            row = 0\n",
        "        if 'west' in name:\n",
        "            col = 0\n",
        "        elif 'east' in name:\n",
        "            col = 400\n",
        "        return adjacency_full[:,row:row+1000,col:col+1000,:]\n",
        "    \n",
        "    if mode == 'trn_val':\n",
        "        trn_adj_0_list = []\n",
        "        trn_adj_1_list = []\n",
        "        trn_adj_2_list = []\n",
        "    \n",
        "        val_adj_0_list = []\n",
        "        val_adj_1_list = []\n",
        "        val_adj_2_list = []\n",
        "\n",
        "        trn_idx = indices[0]\n",
        "        val_idx = indices[1]\n",
        "        for i in tqdm_notebook(range(8)):\n",
        "            adjacency_full = np.load('/content/gdrive/My Drive/adjacency.npz')['dnbr']\n",
        "            adjacency = subset_adjacency(which=i)\n",
        "            \"\"\"binarize to burn or unburnt pixels\"\"\"\n",
        "            adjacency = (adjacency>0.66).astype(int)\n",
        "            \"\"\"Adjacency feature is always taken on timestep prior!!! This is to avoid data leakage!!!\"\"\"\n",
        "            adjacency = np.concatenate([np.zeros((1,1000,1000,1), dtype='int'), adjacency[:-1]],0)\n",
        "            del adjacency_full\n",
        "            gc.collect()\n",
        "            trn_data = Adjacency(adjacency, zone_idx=trn_idx)\n",
        "            val_data = Adjacency(adjacency, zone_idx=val_idx)\n",
        "            del adjacency\n",
        "            gc.collect()\n",
        "            trn_adj_0, trn_adj_1, trn_adj_2 = trn_data.prepare_step_data()\n",
        "            val_adj_0, val_adj_1, val_adj_2 = val_data.prepare_step_data()\n",
        "            del trn_data, val_data\n",
        "            gc.collect()\n",
        "            trn_adj_0_list.append(trn_adj_0)\n",
        "            trn_adj_1_list.append(trn_adj_1)\n",
        "            trn_adj_2_list.append(trn_adj_2)\n",
        "            val_adj_0_list.append(val_adj_0)\n",
        "            val_adj_1_list.append(val_adj_1)\n",
        "            val_adj_2_list.append(val_adj_2)\n",
        "\n",
        "        trn_adj_0_list = np.column_stack(trn_adj_0_list)\n",
        "        trn_adj_1_list = np.column_stack(trn_adj_1_list)\n",
        "        trn_adj_2_list = np.column_stack(trn_adj_2_list)\n",
        "        val_adj_0_list = np.column_stack(val_adj_0_list)\n",
        "        val_adj_1_list = np.column_stack(val_adj_1_list)\n",
        "        val_adj_2_list = np.column_stack(val_adj_2_list)\n",
        "\n",
        "        return trn_adj_0_list, trn_adj_1_list, trn_adj_2_list, val_adj_0_list, val_adj_1_list, val_adj_2_list\n",
        "\n",
        "    elif mode == 'test':\n",
        "        test_adj_0_list = []\n",
        "        test_adj_1_list = []\n",
        "        test_adj_2_list = []\n",
        "\n",
        "        test_idx = indices[0]\n",
        "        for i in tqdm_notebook(range(8)):\n",
        "            adjacency_full = np.load('/content/gdrive/My Drive/adjacency.npz')['dnbr']\n",
        "            adjacency = subset_adjacency(which=i)\n",
        "            adjacency = (adjacency>0.66).astype(int)\n",
        "            adjacency = np.concatenate([np.zeros((1,1000,1000,1), dtype='int'), adjacency[:-1]],0)\n",
        "            del adjacency_full\n",
        "            gc.collect()\n",
        "            test_data = Adjacency(adjacency, zone_idx=test_idx)\n",
        "            del adjacency\n",
        "            gc.collect()\n",
        "            test_adj_0, test_adj_1, test_adj_2 = test_data.prepare_step_data()\n",
        "            del test_data\n",
        "            gc.collect()\n",
        "            test_adj_0_list.append(test_adj_0)\n",
        "            test_adj_1_list.append(test_adj_1)\n",
        "            test_adj_2_list.append(test_adj_2)\n",
        "\n",
        "        test_adj_0_list = np.column_stack(test_adj_0_list)\n",
        "        test_adj_1_list = np.column_stack(test_adj_1_list)\n",
        "        test_adj_2_list = np.column_stack(test_adj_2_list)\n",
        "\n",
        "        return test_adj_0_list, test_adj_1_list, test_adj_2_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrxeDPSiz9nA",
        "colab_type": "code",
        "outputId": "423f28af-44ad-4927-e60c-d5adf8125845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "water_mask = np.load('/content/gdrive/My Drive/water_mask_based_on_ndvi_565.npy')\n",
        "water_mask = np.expand_dims(water_mask, 2)\n",
        "plt.imshow(water_mask[:,:,0], cmap='Greys_r')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7febd51948d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npOaMgNScEZCaMwJSc0ZAas4ISM39H6skPSAMIXjsAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ll9GlCJz9pT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I = 4\n",
        "trn_idx = [i for i in range(25) if i not in range(5*I, 5*I+5)]\n",
        "val_idx = [i for i in range(25) if i in range(5*I, 5*I+5)]\n",
        "\n",
        "imgs = np.load('/content/gdrive/My Drive/cloud_control_data.npz')['imgs']\n",
        "dnbr = np.load('/content/gdrive/My Drive/adjacency.npz')['dnbr']\n",
        "dnbr = dnbr[:,200:1200,200:1200,:]\n",
        "\n",
        "labs = (dnbr>0.66).astype(int)\n",
        "del dnbr\n",
        "gc.collect()\n",
        "## apply water mask\n",
        "for i in range(len(labs)):\n",
        "    labs[i,:][water_mask==1] = 0\n",
        "\n",
        "trn_data = Data(imgs, labs, zone_idx=trn_idx)\n",
        "val_data = Data(imgs, labs, zone_idx=val_idx)\n",
        "\n",
        "del imgs, labs\n",
        "gc.collect()\n",
        "\n",
        "x_trn_0, x_trn_1, x_trn_2, y_trn = trn_data.prepare_step_data()\n",
        "x_val_0, x_val_1, x_val_2, y_val = val_data.prepare_step_data()\n",
        "\n",
        "del trn_data, val_data, water_mask\n",
        "gc.collect()\n",
        "\n",
        "try:\n",
        "    adj_trn_0 = np.load(f'/content/gdrive/My Drive/adjacency_features_{I}.npz')['adj_trn_0']\n",
        "    adj_trn_1 = np.load(f'/content/gdrive/My Drive/adjacency_features_{I}.npz')['adj_trn_1']\n",
        "    adj_trn_2 = np.load(f'/content/gdrive/My Drive/adjacency_features_{I}.npz')['adj_trn_2']\n",
        "\n",
        "    adj_val_0 = np.load(f'/content/gdrive/My Drive/adjacency_features_{I}.npz')['adj_val_0']\n",
        "    adj_val_1 = np.load(f'/content/gdrive/My Drive/adjacency_features_{I}.npz')['adj_val_1']\n",
        "    adj_val_2 = np.load(f'/content/gdrive/My Drive/adjacency_features_{I}.npz')['adj_val_2']\n",
        "\n",
        "except Exception:\n",
        "    adj_trn_0, adj_trn_1, adj_trn_2, adj_val_0, adj_val_1, adj_val_2 = extract_adjacency_feature([trn_idx, val_idx])\n",
        "    np.savez_compressed(f'/content/gdrive/My Drive/adjacency_features_{I}', adj_trn_0=adj_trn_0, adj_trn_1=adj_trn_1, adj_trn_2=adj_trn_2, adj_val_0=adj_val_0, adj_val_1=adj_val_1, adj_val_2=adj_val_2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4_1FrXpz8Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gsod = pd.read_csv('/content/gdrive/My Drive/flinders_chase_clean_5day.csv').values\n",
        "gsod = np.array(gsod, dtype='float')\n",
        "## normalize\n",
        "gsod = gsod/gsod.max(axis=0)\n",
        "\n",
        "num_trn_0, num_trn_1, num_trn_2 = gsod[:-2,:], gsod[1:-1,:], gsod[2:,:10]\n",
        "num_val_0, num_val_1, num_val_2 = gsod[:-2,:], gsod[1:-1,:], gsod[2:,:10]\n",
        "for i in range(len(trn_idx)-1):\n",
        "  num_trn_0 = np.concatenate([num_trn_0, gsod[:-2,:]])\n",
        "  num_trn_1 = np.concatenate([num_trn_1, gsod[1:-1,:]])\n",
        "  ### the last series should only have day_0 data!\n",
        "  num_trn_2 = np.concatenate([num_trn_2, gsod[2:,:10]])\n",
        "for i in range(len(val_idx)-1):\n",
        "  num_val_0 = np.concatenate([num_val_0, gsod[:-2,:]])\n",
        "  num_val_1 = np.concatenate([num_val_1, gsod[1:-1,:]])\n",
        "  ### the last series should only have day_0 data!\n",
        "  num_val_2 = np.concatenate([num_val_2, gsod[2:,:10]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46XUyk6Oz8FF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(y_true, y_pred, w=20):\n",
        "    crossentropy_loss = K.binary_crossentropy(y_true, y_pred)\n",
        "    weight  = (y_true*(w-1)) + 1\n",
        "    return crossentropy_loss*(weight)*(1/w)\n",
        "\n",
        "def f2_loss(y_true, y_pred, beta=2):\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "    f_score = (1+beta**2) * p * r / ( (beta**2) * p + r + K.epsilon())\n",
        "    f_score = tf.where(tf.is_nan(f_score), tf.zeros_like(f_score), f_score)\n",
        "    return 1 - K.mean(f_score)\n",
        "\n",
        "def f4_loss(y_true, y_pred, beta=4):\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "    f_score = (1+beta**2) * p * r / ( (beta**2) * p + r + K.epsilon())\n",
        "    f_score = tf.where(tf.is_nan(f_score), tf.zeros_like(f_score), f_score)\n",
        "    return 1 - K.mean(f_score)\n",
        "def custom_loss(y_true, y_pred, w=20):\n",
        "    crossentropy_loss = K.binary_crossentropy(y_true, y_pred)\n",
        "    weight  = (y_true*(w-1)) + 1\n",
        "    return crossentropy_loss*(weight)*(1/w)\n",
        "\n",
        "def f2_loss(y_true, y_pred, beta=2):\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "    f_score = (1+beta**2) * p * r / ( (beta**2) * p + r + K.epsilon())\n",
        "    f_score = tf.where(tf.is_nan(f_score), tf.zeros_like(f_score), f_score)\n",
        "    return 1 - K.mean(f_score)\n",
        "\n",
        "def f4_loss(y_true, y_pred, beta=4):\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "    f_score = (1+beta**2) * p * r / ( (beta**2) * p + r + K.epsilon())\n",
        "    f_score = tf.where(tf.is_nan(f_score), tf.zeros_like(f_score), f_score)\n",
        "    return 1 - K.mean(f_score)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NwkDddnz8Hu",
        "colab_type": "code",
        "outputId": "fc3d93bb-0948-48ad-b02c-e56b50352983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "random_everything(seed=42)\n",
        "#model = Model(inputs=[img_0, img_1, img_2, num_0, num_1, num_2, adj_0, adj_1, adj_2], output=dense)\n",
        "\n",
        "model.compile(optimizer = Adam(lr=2e-5, decay=1e-6), \n",
        "              #loss=custom_loss,\n",
        "              loss=f4_loss,\n",
        "              metrics=['acc', recall_m, precision_m, f1_m, f2_m, f3_m, f4_m])\n",
        "\n",
        "ckpt = ModelCheckpoint(f\"/content/gdrive/My Drive/CNN_LSTM_{I}_final.hdf5\", monitor = \"val_f4_m\", save_best_only = True, mode = \"max\", save_weights_only=True)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "history = model.fit([x_trn_0, x_trn_1, x_trn_2, num_trn_0, num_trn_1, num_trn_2, adj_trn_0, adj_trn_1, adj_trn_2], y_trn,\n",
        "                        validation_data=([x_val_0, x_val_1, x_val_2, num_val_0, num_val_1, num_val_2, adj_val_0, adj_val_1, adj_val_2], y_val),\n",
        "                        batch_size=24, epochs=100,\n",
        "                        callbacks=[es, ckpt, \n",
        "                                   keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-8 )\n",
        "                                   ]\n",
        "                        )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-8-eaf77e576dcc>:51: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 2120 samples, validate on 530 samples\n",
            "Epoch 1/100\n",
            "2120/2120 [==============================] - 102s 48ms/step - loss: 0.7386 - acc: 0.0434 - recall_m: 0.6679 - precision_m: 0.0425 - f1_m: 0.0790 - f2_m: 0.1641 - f3_m: 0.2587 - f4_m: 0.3420 - val_loss: 0.6940 - val_acc: 0.0491 - val_recall_m: 0.6377 - val_precision_m: 0.0491 - val_f1_m: 0.0898 - val_f2_m: 0.1817 - val_f3_m: 0.2790 - val_f4_m: 0.3601\n",
            "Epoch 2/100\n",
            "2120/2120 [==============================] - 77s 36ms/step - loss: 0.7078 - acc: 0.0425 - recall_m: 0.6226 - precision_m: 0.0425 - f1_m: 0.0784 - f2_m: 0.1606 - f3_m: 0.2500 - f4_m: 0.3272 - val_loss: 0.6636 - val_acc: 0.0491 - val_recall_m: 0.6377 - val_precision_m: 0.0491 - val_f1_m: 0.0898 - val_f2_m: 0.1817 - val_f3_m: 0.2790 - val_f4_m: 0.3601\n",
            "Epoch 3/100\n",
            "2120/2120 [==============================] - 77s 36ms/step - loss: 0.6809 - acc: 0.0425 - recall_m: 0.6377 - precision_m: 0.0425 - f1_m: 0.0784 - f2_m: 0.1606 - f3_m: 0.2506 - f4_m: 0.3292 - val_loss: 0.6483 - val_acc: 0.0491 - val_recall_m: 0.6377 - val_precision_m: 0.0491 - val_f1_m: 0.0898 - val_f2_m: 0.1817 - val_f3_m: 0.2790 - val_f4_m: 0.3601\n",
            "Epoch 4/100\n",
            "2120/2120 [==============================] - 77s 36ms/step - loss: 0.6577 - acc: 0.0429 - recall_m: 0.6377 - precision_m: 0.0425 - f1_m: 0.0785 - f2_m: 0.1613 - f3_m: 0.2519 - f4_m: 0.3310 - val_loss: 0.6353 - val_acc: 0.0491 - val_recall_m: 0.6377 - val_precision_m: 0.0491 - val_f1_m: 0.0898 - val_f2_m: 0.1817 - val_f3_m: 0.2790 - val_f4_m: 0.3601\n",
            "Epoch 5/100\n",
            "2120/2120 [==============================] - 77s 36ms/step - loss: 0.6354 - acc: 0.0665 - recall_m: 0.6491 - precision_m: 0.0443 - f1_m: 0.0815 - f2_m: 0.1666 - f3_m: 0.2594 - f4_m: 0.3400 - val_loss: 0.6308 - val_acc: 0.1679 - val_recall_m: 0.6377 - val_precision_m: 0.0534 - val_f1_m: 0.0973 - val_f2_m: 0.1946 - val_f3_m: 0.2949 - val_f4_m: 0.3763\n",
            "Epoch 6/100\n",
            "2120/2120 [==============================] - 77s 36ms/step - loss: 0.6216 - acc: 0.3736 - recall_m: 0.6453 - precision_m: 0.0694 - f1_m: 0.1213 - f2_m: 0.2266 - f3_m: 0.3266 - f4_m: 0.4043 - val_loss: 0.6193 - val_acc: 0.2925 - val_recall_m: 0.6377 - val_precision_m: 0.0614 - val_f1_m: 0.1106 - val_f2_m: 0.2155 - val_f3_m: 0.3185 - val_f4_m: 0.3986\n",
            "Epoch 7/100\n",
            "2120/2120 [==============================] - 76s 36ms/step - loss: 0.6146 - acc: 0.5873 - recall_m: 0.6226 - precision_m: 0.1058 - f1_m: 0.1733 - f2_m: 0.2913 - f3_m: 0.3874 - f4_m: 0.4538 - val_loss: 0.5936 - val_acc: 0.4491 - val_recall_m: 0.6377 - val_precision_m: 0.0716 - val_f1_m: 0.1273 - val_f2_m: 0.2414 - val_f3_m: 0.3473 - val_f4_m: 0.4255\n",
            "Epoch 8/100\n",
            "2120/2120 [==============================] - 76s 36ms/step - loss: 0.5679 - acc: 0.7241 - recall_m: 0.6585 - precision_m: 0.1355 - f1_m: 0.2174 - f2_m: 0.3529 - f3_m: 0.4539 - f4_m: 0.5180 - val_loss: 0.5760 - val_acc: 0.5925 - val_recall_m: 0.6226 - val_precision_m: 0.1057 - val_f1_m: 0.1761 - val_f2_m: 0.3024 - val_f3_m: 0.4031 - val_f4_m: 0.4696\n",
            "Epoch 9/100\n",
            "2120/2120 [==============================] - 76s 36ms/step - loss: 0.5550 - acc: 0.7863 - recall_m: 0.6142 - precision_m: 0.1858 - f1_m: 0.2642 - f2_m: 0.3787 - f3_m: 0.4580 - f4_m: 0.5071 - val_loss: 0.5993 - val_acc: 0.6283 - val_recall_m: 0.5547 - val_precision_m: 0.0897 - val_f1_m: 0.1521 - val_f2_m: 0.2643 - val_f3_m: 0.3541 - val_f4_m: 0.4138\n",
            "Epoch 10/100\n",
            "2120/2120 [==============================] - 76s 36ms/step - loss: 0.5093 - acc: 0.8608 - recall_m: 0.6019 - precision_m: 0.2606 - f1_m: 0.3382 - f2_m: 0.4412 - f3_m: 0.5031 - f4_m: 0.5374 - val_loss: 0.5539 - val_acc: 0.8811 - val_recall_m: 0.5170 - val_precision_m: 0.2106 - val_f1_m: 0.2847 - val_f2_m: 0.3780 - val_f3_m: 0.4325 - val_f4_m: 0.4622\n",
            "Epoch 11/100\n",
            "2120/2120 [==============================] - 76s 36ms/step - loss: 0.4816 - acc: 0.8976 - recall_m: 0.6075 - precision_m: 0.3332 - f1_m: 0.4091 - f2_m: 0.4959 - f3_m: 0.5422 - f4_m: 0.5660 - val_loss: 0.5976 - val_acc: 0.9283 - val_recall_m: 0.4264 - val_precision_m: 0.2141 - val_f1_m: 0.2780 - val_f2_m: 0.3469 - val_f3_m: 0.3813 - val_f4_m: 0.3983\n",
            "Epoch 12/100\n",
            "2120/2120 [==============================] - 76s 36ms/step - loss: 0.4368 - acc: 0.9340 - recall_m: 0.6330 - precision_m: 0.4185 - f1_m: 0.4815 - f2_m: 0.5516 - f3_m: 0.5868 - f4_m: 0.6042 - val_loss: 0.5053 - val_acc: 0.9264 - val_recall_m: 0.5547 - val_precision_m: 0.2970 - val_f1_m: 0.3754 - val_f2_m: 0.4590 - val_f3_m: 0.5004 - val_f4_m: 0.5208\n",
            "Epoch 13/100\n",
            "2120/2120 [==============================] - 76s 36ms/step - loss: 0.4545 - acc: 0.9222 - recall_m: 0.5962 - precision_m: 0.3796 - f1_m: 0.4402 - f2_m: 0.5094 - f3_m: 0.5458 - f4_m: 0.5643 - val_loss: 0.5405 - val_acc: 0.8811 - val_recall_m: 0.5170 - val_precision_m: 0.2316 - val_f1_m: 0.3024 - val_f2_m: 0.3902 - val_f3_m: 0.4405 - val_f4_m: 0.4677\n",
            "Epoch 14/100\n",
            "2120/2120 [==============================] - 76s 36ms/step - loss: 0.4797 - acc: 0.9104 - recall_m: 0.5830 - precision_m: 0.3329 - f1_m: 0.4014 - f2_m: 0.4807 - f3_m: 0.5231 - f4_m: 0.5449 - val_loss: 0.4689 - val_acc: 0.8698 - val_recall_m: 0.6000 - val_precision_m: 0.2418 - val_f1_m: 0.3296 - val_f2_m: 0.4422 - val_f3_m: 0.5065 - val_f4_m: 0.5403\n",
            "Epoch 15/100\n",
            "2120/2120 [==============================] - 74s 35ms/step - loss: 0.4021 - acc: 0.9000 - recall_m: 0.6877 - precision_m: 0.3262 - f1_m: 0.4221 - f2_m: 0.5359 - f3_m: 0.5980 - f4_m: 0.6304 - val_loss: 0.4760 - val_acc: 0.9057 - val_recall_m: 0.5849 - val_precision_m: 0.2942 - val_f1_m: 0.3737 - val_f2_m: 0.4670 - val_f3_m: 0.5166 - val_f4_m: 0.5418\n",
            "Epoch 16/100\n",
            "2120/2120 [==============================] - 74s 35ms/step - loss: 0.4209 - acc: 0.9325 - recall_m: 0.6208 - precision_m: 0.4108 - f1_m: 0.4777 - f2_m: 0.5462 - f3_m: 0.5790 - f4_m: 0.5948 - val_loss: 0.5421 - val_acc: 0.9264 - val_recall_m: 0.4868 - val_precision_m: 0.2756 - val_f1_m: 0.3307 - val_f2_m: 0.3997 - val_f3_m: 0.4364 - val_f4_m: 0.4551\n",
            "Epoch 17/100\n",
            "2120/2120 [==============================] - 74s 35ms/step - loss: 0.4154 - acc: 0.9250 - recall_m: 0.6340 - precision_m: 0.3904 - f1_m: 0.4643 - f2_m: 0.5434 - f3_m: 0.5825 - f4_m: 0.6019 - val_loss: 0.4697 - val_acc: 0.9038 - val_recall_m: 0.5774 - val_precision_m: 0.2727 - val_f1_m: 0.3456 - val_f2_m: 0.4458 - val_f3_m: 0.5009 - val_f4_m: 0.5291\n",
            "Epoch 18/100\n",
            "2120/2120 [==============================] - 73s 35ms/step - loss: 0.3550 - acc: 0.9354 - recall_m: 0.6868 - precision_m: 0.4513 - f1_m: 0.5192 - f2_m: 0.5951 - f3_m: 0.6340 - f4_m: 0.6535 - val_loss: 0.5141 - val_acc: 0.9660 - val_recall_m: 0.5019 - val_precision_m: 0.4294 - val_f1_m: 0.4357 - val_f2_m: 0.4643 - val_f3_m: 0.4804 - val_f4_m: 0.4885\n",
            "Epoch 19/100\n",
            "2120/2120 [==============================] - 74s 35ms/step - loss: 0.4236 - acc: 0.9491 - recall_m: 0.6030 - precision_m: 0.4779 - f1_m: 0.5146 - f2_m: 0.5562 - f3_m: 0.5767 - f4_m: 0.5867 - val_loss: 0.5068 - val_acc: 0.9623 - val_recall_m: 0.5245 - val_precision_m: 0.4068 - val_f1_m: 0.4402 - val_f2_m: 0.4816 - val_f3_m: 0.5010 - val_f4_m: 0.5101\n",
            "Epoch 20/100\n",
            "2120/2120 [==============================] - 73s 35ms/step - loss: 0.4185 - acc: 0.9392 - recall_m: 0.6066 - precision_m: 0.4500 - f1_m: 0.4956 - f2_m: 0.5472 - f3_m: 0.5730 - f4_m: 0.5857 - val_loss: 0.5164 - val_acc: 0.9623 - val_recall_m: 0.5019 - val_precision_m: 0.3389 - val_f1_m: 0.3949 - val_f2_m: 0.4489 - val_f3_m: 0.4730 - val_f4_m: 0.4843\n",
            "Epoch 21/100\n",
            "2120/2120 [==============================] - 73s 35ms/step - loss: 0.3988 - acc: 0.9415 - recall_m: 0.6340 - precision_m: 0.4481 - f1_m: 0.5068 - f2_m: 0.5677 - f3_m: 0.5969 - f4_m: 0.6110 - val_loss: 0.5386 - val_acc: 0.9604 - val_recall_m: 0.4792 - val_precision_m: 0.3563 - val_f1_m: 0.3892 - val_f2_m: 0.4317 - val_f3_m: 0.4526 - val_f4_m: 0.4627\n",
            "Epoch 22/100\n",
            "2120/2120 [==============================] - 73s 35ms/step - loss: 0.3449 - acc: 0.9590 - recall_m: 0.6868 - precision_m: 0.4962 - f1_m: 0.5543 - f2_m: 0.6170 - f3_m: 0.6476 - f4_m: 0.6625 - val_loss: 0.5393 - val_acc: 0.9604 - val_recall_m: 0.4792 - val_precision_m: 0.3540 - val_f1_m: 0.3874 - val_f2_m: 0.4309 - val_f3_m: 0.4523 - val_f4_m: 0.4626\n",
            "Epoch 23/100\n",
            "2120/2120 [==============================] - 73s 35ms/step - loss: 0.3422 - acc: 0.9637 - recall_m: 0.6915 - precision_m: 0.5396 - f1_m: 0.5833 - f2_m: 0.6325 - f3_m: 0.6577 - f4_m: 0.6703 - val_loss: 0.5237 - val_acc: 0.9604 - val_recall_m: 0.5019 - val_precision_m: 0.3162 - val_f1_m: 0.3783 - val_f2_m: 0.4394 - val_f3_m: 0.4675 - val_f4_m: 0.4807\n",
            "Epoch 24/100\n",
            "2120/2120 [==============================] - 73s 35ms/step - loss: 0.3811 - acc: 0.9604 - recall_m: 0.6425 - precision_m: 0.5066 - f1_m: 0.5488 - f2_m: 0.5935 - f3_m: 0.6150 - f4_m: 0.6255 - val_loss: 0.5195 - val_acc: 0.9642 - val_recall_m: 0.5019 - val_precision_m: 0.3358 - val_f1_m: 0.3917 - val_f2_m: 0.4464 - val_f3_m: 0.4714 - val_f4_m: 0.4832\n",
            "Epoch 00024: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRyod5Y5z5nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(f\"/content/gdrive/My Drive/CNN_LSTM_{I}_final.hdf5\")\n",
        "preds = model.predict([x_val_0, x_val_1, x_val_2, num_val_0, num_val_1, num_val_2, adj_val_0, adj_val_1, adj_val_2])\n",
        "#np.save(f'/content/gdrive/My Drive/preds_{I}', preds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0UOByGUz5pm",
        "colab_type": "code",
        "outputId": "a4fdae07-83ed-4a2f-a0de-b06613717342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, fbeta_score, cohen_kappa_score\n",
        "\n",
        "print(accuracy_score(y_val, preds.round().flatten()))\n",
        "print(f1_score(y_val, preds.round().flatten()))\n",
        "print(fbeta_score(y_val, preds.round().flatten(),2))\n",
        "print(fbeta_score(y_val, preds.round().flatten(),3))\n",
        "print(fbeta_score(y_val, preds.round().flatten(),4))\n",
        "print(cohen_kappa_score(y_val, preds.round().flatten()))\n",
        "print(recall_score(y_val, preds.round().flatten()))\n",
        "print(precision_score(y_val, preds.round().flatten()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9056603773584906\n",
            "0.47916666666666663\n",
            "0.660919540229885\n",
            "0.756578947368421\n",
            "0.8045267489711934\n",
            "0.4390347163420829\n",
            "0.8846153846153846\n",
            "0.32857142857142857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUvZP00YxmhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_imgs_0 = np.load('/content/gdrive/My Drive/test_data.npz')['test_imgs_0']\n",
        "test_imgs_1 = np.load('/content/gdrive/My Drive/test_data.npz')['test_imgs_1']\n",
        "test_imgs_2 = np.load('/content/gdrive/My Drive/test_data.npz')['test_imgs_2']\n",
        "test_adj_0 = np.load('/content/gdrive/My Drive/test_data.npz')['test_adj_0']\n",
        "test_adj_1 = np.load('/content/gdrive/My Drive/test_data.npz')['test_adj_1']\n",
        "test_adj_2 = np.load('/content/gdrive/My Drive/test_data.npz')['test_adj_2']\n",
        "test_labs = np.load('/content/gdrive/My Drive/test_data.npz')['test_lab']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTVsPCLjyDCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_gsod = pd.read_csv('/content/gdrive/My Drive/test_gsod_final.csv').values\n",
        "test_gsod_0, test_gsod_1, test_gsod_2 = test_gsod[:-2,:], test_gsod[1:-1,:], test_gsod[2:,:10]\n",
        "for i in range(1,25):\n",
        "    test_gsod_0 = np.concatenate([test_gsod_0, test_gsod[:-2,:]])\n",
        "    test_gsod_1 = np.concatenate([test_gsod_1, test_gsod[1:-1,:]])\n",
        "    test_gsod_2 = np.concatenate([test_gsod_2, test_gsod[2:,:10]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24g1qG6Byuc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I = 4\n",
        "model.load_weights(f\"/content/gdrive/My Drive/CNN_LSTM_{I}_final.hdf5\")\n",
        "test_preds = model.predict([test_imgs_0, test_imgs_1, test_imgs_2, test_gsod_0, test_gsod_1, test_gsod_2, test_adj_0, test_adj_1, test_adj_2])\n",
        "np.save(f'/content/gdrive/My Drive/test_preds_{I}', test_preds.flatten())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRvi2NIu41vz",
        "colab_type": "code",
        "outputId": "45657a7b-f2f6-4433-a2c2-68d8eeae8fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, fbeta_score, cohen_kappa_score\n",
        "\n",
        "print(accuracy_score(test_labs, test_preds.round().flatten()))\n",
        "print(f1_score(test_labs, test_preds.round().flatten()))\n",
        "print(fbeta_score(test_labs, test_preds.round().flatten(),2))\n",
        "print(fbeta_score(test_labs, test_preds.round().flatten(),3))\n",
        "print(fbeta_score(test_labs, test_preds.round().flatten(),4))\n",
        "print(cohen_kappa_score(test_labs, test_preds.round().flatten()))\n",
        "print(recall_score(test_labs, test_preds.round().flatten()))\n",
        "print(precision_score(test_labs, test_preds.round().flatten()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.68\n",
            "0.5555555555555556\n",
            "0.4807692307692307\n",
            "0.460122699386503\n",
            "0.4521276595744681\n",
            "0.33085501858736055\n",
            "0.4411764705882353\n",
            "0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C1aG3b75JAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}